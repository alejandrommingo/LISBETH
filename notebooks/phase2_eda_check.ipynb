{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Phase 2 Data Verification & EDA\n",
                "\n",
                "This notebook verifies the integrity of Phase 2 output data against Phase 3 requirements."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import pyarrow.parquet as pq\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from datetime import datetime\n",
                "\n",
                "# Set up visual style\n",
                "plt.style.use('seaborn-v0_8')\n",
                "sns.set_context(\"talk\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "FILES = {\n",
                "    'Canonical (Target)': '../data/embeddings_occurrences.parquet',\n",
                "    'Baseline (Old)': '../data/embeddings_baseline.parquet',\n",
                "    'DAPT (Old)': '../data/embeddings_dapt.parquet'\n",
                "}\n",
                "\n",
                "def inspect_data(name, path):\n",
                "    print(f\"\\n{'='*20} INSPECTING: {name} {'='*20}\")\n",
                "    print(f\"Loading {path}...\")\n",
                "    try:\n",
                "        df = pd.read_parquet(path)\n",
                "    except Exception as e:\n",
                "        print(f\"FAILED to load parquet: {e}\")\n",
                "        return None\n",
                "    \n",
                "    print(f\"Shape: {df.shape}\")\n",
                "    print(\"\\nColumns:\")\n",
                "    print(df.columns.tolist())\n",
                "    \n",
                "    print(\"\\nSample (first 3 rows):\")\n",
                "    display(df.head(3))\n",
                "    \n",
                "    print(\"\\nSchema & Types:\")\n",
                "    print(df.dtypes)\n",
                "    \n",
                "    return df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def verify_requirements(df):\n",
                "    print(\"\\n--- VERIFYING REQUIREMENTS (Canonical) ---\")\n",
                "    \n",
                "    # 2.2 A) Identificación\n",
                "    req_cols_a = ['occurrence_id', 'run_id', 'model_id', 'model_variant', 'layer_strategy', 'pooling_strategy']\n",
                "    missing_a = [c for c in req_cols_a if c not in df.columns]\n",
                "    if missing_a:\n",
                "        print(f\"❌ MISSING A) Identificación columns: {missing_a}\")\n",
                "    else:\n",
                "        print(\"✅ A) Identificación columns present\")\n",
                "        \n",
                "    # 2.2 B) Información temporal\n",
                "    req_cols_b = ['published_at', 'year', 'month', 'year_month']\n",
                "    missing_b = [c for c in req_cols_b if c not in df.columns]\n",
                "    if missing_b:\n",
                "         print(f\"❌ MISSING B) Temporal columns: {missing_b}\")\n",
                "    else:\n",
                "        print(\"✅ B) Temporal columns present\")\n",
                "        \n",
                "    # 2.2 C) Fuente\n",
                "    req_cols_c = ['newspaper', 'source_api', 'url']\n",
                "    missing_c = [c for c in req_cols_c if c not in df.columns]\n",
                "    if missing_c:\n",
                "        print(f\"❌ MISSING C) Source columns: {missing_c}\")\n",
                "    else:\n",
                "        print(\"✅ C) Source columns present\")\n",
                "        \n",
                "    # 2.2 D) Lingüística (CRITICO)\n",
                "    req_cols_d = ['keyword_canonical', 'keyword_matched', 'char_start', 'char_end', 'token_start', 'token_end']\n",
                "    missing_d = [c for c in req_cols_d if c not in df.columns]\n",
                "    if missing_d:\n",
                "        print(f\"❌ MISSING D) Linguistic columns: {missing_d}\")\n",
                "    else:\n",
                "        print(\"✅ D) Linguistic columns present\")\n",
                "\n",
                "    # 2.2 E) Contexto\n",
                "    req_cols_e = ['context_sentence'] # context_window is optional\n",
                "    missing_e = [c for c in req_cols_e if c not in df.columns]\n",
                "    if missing_e:\n",
                "        print(f\"❌ MISSING E) Context columns: {missing_e}\")\n",
                "    else:\n",
                "        print(\"✅ E) Context columns present\")\n",
                "\n",
                "    # 2.2 F) Embeddings\n",
                "    req_cols_f = ['embedding_contextual_last4', 'embedding_contextual_penultimate']\n",
                "    missing_f = [c for c in req_cols_f if c not in df.columns]\n",
                "    if missing_f:\n",
                "        print(f\"❌ MISSING F) Embedding columns: {missing_f}\")\n",
                "    else:\n",
                "        print(\"✅ F) Embedding columns present\")\n",
                "        \n",
                "    # 3. Validation Logic\n",
                "    print(\"\\n--- VALIDATION CHECKS ---\")\n",
                "    \n",
                "    # Check nulls in critical columns\n",
                "    critical_cols = req_cols_a + req_cols_b + req_cols_c + req_cols_d + req_cols_e + req_cols_f\n",
                "    present_critical = [c for c in critical_cols if c in df.columns]\n",
                "    nulls = df[present_critical].isnull().sum()\n",
                "    if nulls.sum() > 0:\n",
                "        print(\"⚠️ NULLS FOUND in critical columns:\")\n",
                "        print(nulls[nulls > 0])\n",
                "    else:\n",
                "        print(\"✅ No nulls in critical columns\")\n",
                "        \n",
                "    # Check date ranges\n",
                "    if 'published_at' in df.columns:\n",
                "        print(f\"\\nDate Range: {df['published_at'].min()} to {df['published_at'].max()}\")\n",
                "        \n",
                "    # Check Unique Model Variants\n",
                "    if 'model_variant' in df.columns:\n",
                "        print(\"\\nModel Variants distribution:\")\n",
                "        print(df['model_variant'].value_counts())\n",
                "        \n",
                "    # Check Embedding Dimensions\n",
                "    if 'embedding_contextual_last4' in df.columns:\n",
                "        # Check first non-null\n",
                "        sample_emb = df['embedding_contextual_last4'].dropna().iloc[0]\n",
                "        print(f\"\\nEmbedding Last4 Shape: {len(sample_emb)}\")\n",
                "        \n",
                "    if 'embedding_contextual_penultimate' in df.columns:\n",
                "         s_series = df['embedding_contextual_penultimate'].dropna()\n",
                "         if len(s_series) > 0:\n",
                "             sample_emb = s_series.iloc[0]\n",
                "             print(f\"Embedding Penultimate Shape: {len(sample_emb)}\")\n",
                "         else:\n",
                "             print(\"❌ Embedding Penultimate is ALL NULL\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def advanced_eda(df):\n",
                "    print(\"\\n--- ADVANCED EDA PLOTS ---\")\n",
                "    \n",
                "    # Temporal Distribution\n",
                "    if 'year_month' in df.columns:\n",
                "        plt.figure(figsize=(12, 6))\n",
                "        df['year_month'].sort_values().value_counts(sort=False).plot(kind='bar')\n",
                "        plt.title(\"Distribution of Occurrences by Year-Month\")\n",
                "        plt.xticks(rotation=45)\n",
                "        plt.tight_layout()\n",
                "        plt.show()\n",
                "        \n",
                "    # Model x Layer Strategy\n",
                "    if 'model_variant' in df.columns and 'layer_strategy' in df.columns:\n",
                "        plt.figure(figsize=(10, 6))\n",
                "        sns.countplot(data=df, x='model_variant', hue='layer_strategy')\n",
                "        plt.title(\"Count of Occurrences by Model Variant and Layer Strategy\")\n",
                "        plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# EXECUTION\n",
                "dfs = {}\n",
                "for name, path in FILES.items():\n",
                "    df = inspect_data(name, path)\n",
                "    if df is not None:\n",
                "        dfs[name] = df\n",
                "        if name == 'Canonical (Target)':\n",
                "            verify_requirements(df)\n",
                "            advanced_eda(df)\n",
                "        else:\n",
                "             print(f\"Skipping strict verification for {name} (Old Schema predicted)\")\n",
                "             if 'model_variant' in df.columns:\n",
                "                 print(f\"Model Variants: {df['model_variant'].unique()}\")\n",
                "             \n",
                "             # Check for null dates which was a finding\n",
                "             if 'published_at' in df.columns:\n",
                "                 print(f\"Null Dates: {df['published_at'].isnull().sum()} / {len(df)}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}