{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Tutorial Fase 2: Infraestructura NLP para Análisis Sociológico\n",
                "\n",
                "**Proyecto**: *De la billetera móvil al Actor Social (Lisbeth)*\n",
                "**Objetivo**: Establecer los cimientos computacionales para analizar cómo evoluciona el significado de \"Yape\" en la prensa peruana (2016-2023).\n",
                "\n",
                "---\n",
                "\n",
                "Este notebook explica paso a paso la **metodología técnica** que estamos implementando. Para esta demostración, **crearemos un entorno aislado (sandbox)** con datos de muestra, asegurando que la ejecución sea rápida y no afecte a los modelos o datos reales del proyecto.\n",
                "\n",
                "### Conceptos Clave\n",
                "1.  **DAPT (Domain-Adaptive Pretraining)**: Enseñamos al modelo el \"habla peruana\".\n",
                "2.  **Subword Pooling**: Reconstruimos matemáticamente palabras compuestas (\"Yapear\").\n",
                "3.  **Embeddings Contextuales**: Capturamos el significado en uso."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Configuración del Entorno y Sandbox\n",
                "Configuramos las rutas y preparamos una carpeta temporal `demo_sandbox` para nuestros experimentos."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "import os\n",
                "import shutil\n",
                "import pandas as pd\n",
                "import torch\n",
                "import glob\n",
                "\n",
                "# 1. Ajuste robusto del PATH para importar src\n",
                "# Si estamos en 'notebooks/', subimos un nivel. Si estamos en root, nos quedamos.\n",
                "if os.path.basename(os.getcwd()) == \"notebooks\":\n",
                "    PROJECT_ROOT = os.path.abspath(\"..\")\n",
                "else:\n",
                "    PROJECT_ROOT = os.getcwd()\n",
                "\n",
                "if PROJECT_ROOT not in sys.path:\n",
                "    sys.path.append(PROJECT_ROOT)\n",
                "\n",
                "# Importamos módulos del proyecto\n",
                "from src.nlp.model import LisbethModel\n",
                "from src.nlp.dapt import dapt\n",
                "from src.nlp.extract import extract_embeddings\n",
                "\n",
                "# 2. Crear entorno Sandbox (Carpetas temporales)\n",
                "SANDBOX_DIR = os.path.join(PROJECT_ROOT, \"demo_sandbox\")\n",
                "DATA_DEMO = os.path.join(SANDBOX_DIR, \"data\")\n",
                "MODELS_DEMO = os.path.join(SANDBOX_DIR, \"models\")\n",
                "\n",
                "# Limpiamos y recreamos el sandbox\n",
                "if os.path.exists(SANDBOX_DIR):\n",
                "    shutil.rmtree(SANDBOX_DIR)\n",
                "os.makedirs(DATA_DEMO, exist_ok=True)\n",
                "os.makedirs(MODELS_DEMO, exist_ok=True)\n",
                "\n",
                "print(f\"Entorno configurado en: {SANDBOX_DIR}\")\n",
                "print(f\"Torch disponible: {torch.cuda.is_available()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Preparación de Datos de Muestra\n",
                "En lugar de procesar los gigabytes de datos reales, tomaremos una pequeña muestra de los CSV originales para demostrar el funcionamiento."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Localizar datos reales\n",
                "REAL_DATA_DIR = os.path.join(PROJECT_ROOT, \"data\")\n",
                "csv_files = glob.glob(os.path.join(REAL_DATA_DIR, \"yape_*.csv\"))\n",
                "\n",
                "if csv_files:\n",
                "    # Tomamos el primer archivo y leemos solo 20 filas\n",
                "    source_csv = csv_files[0]\n",
                "    print(f\"Creando muestra desde: {os.path.basename(source_csv)}\")\n",
                "    \n",
                "    df_sample = pd.read_csv(source_csv, nrows=20)\n",
                "    \n",
                "    # Guardamos como 'yape_demo.csv' en la carpeta demo\n",
                "    sample_csv_path = os.path.join(DATA_DEMO, \"yape_demo.csv\")\n",
                "    df_sample.to_csv(sample_csv_path, index=False)\n",
                "    \n",
                "    # Creamos también un mini corpus de texto para DAPT\n",
                "    corpus_sample_path = os.path.join(DATA_DEMO, \"corpus_sample.txt\")\n",
                "    with open(corpus_sample_path, \"w\") as f:\n",
                "        if \"plain_text\" in df_sample.columns:\n",
                "            for text in df_sample[\"plain_text\"].dropna():\n",
                "                f.write(str(text).replace(\"\\n\", \" \") + \"\\n\")\n",
                "    print(\"Datos de muestra generados exitosamente.\")\n",
                "else:\n",
                "    print(\"ADVERTENCIA: No se encontraron datos reales en 'data/'. Se usarán datos sintéticos.\")\n",
                "    # Crear dato sintético si no hay CSVs\n",
                "    corpus_sample_path = os.path.join(DATA_DEMO, \"corpus_sample.txt\")\n",
                "    with open(corpus_sample_path, \"w\") as f:\n",
                "        f.write(\"El aplicativo Yape permite yapear dinero fasilmente.\\n\" * 10)\n",
                "    sample_csv_path = os.path.join(DATA_DEMO, \"yape_demo.csv\")\n",
                "    pd.DataFrame({\"plain_text\": [\"Yape es genial.\"]*10, \"published_at\": [\"2023-01-01\"]*10, \"newspaper\": [\"Demo\"]*10}).to_csv(sample_csv_path, index=False)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Tokenización y Subword Pooling\n",
                "Probamos cómo el modelo maneja palabras clave como \"Yapear\". Usamos `xlm-roberta-base` para esta demo por su ligereza y compatibilidad."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "MODEL_NAME = \"xlm-roberta-base\"\n",
                "model = LisbethModel(model_name=MODEL_NAME)\n",
                "\n",
                "frase = \"Voy a yapear el pago.\"\n",
                "tokens = model.tokenizer.tokenize(frase)\n",
                "\n",
                "print(f\"Frase: '{frase}'\")\n",
                "print(f\"Tokens: {tokens}\")\n",
                "\n",
                "# Extracción con pooling automático\n",
                "try:\n",
                "    embeddings = model.extract_embedding(frase, \"yapear\")\n",
                "    if embeddings:\n",
                "        print(f\"Vector generado correctamente. Dimensión: {embeddings[0].shape}\")\n",
                "    else:\n",
                "        print(\"No se pudo extraer el vector.\")\n",
                "except Exception as e:\n",
                "    print(f\"Error en extracción: {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Ejecución del Pipeline Reducido\n",
                "\n",
                "### A. Domain Adaptive Pretraining (Simulado)\n",
                "Entrenamos el modelo por 1 sola época usando nuestro mini-corpus. El resultado se guarda en `demo_sandbox/models`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "try:\n",
                "    print(\"Entrenando modelo adaptado (Demo)...\")\n",
                "    dapt(MODEL_NAME, corpus_sample_path, MODELS_DEMO, epochs=1)\n",
                "    print(f\"✅ Modelo guardado en: {MODELS_DEMO}\")\n",
                "except Exception as e:\n",
                "    print(f\"❌ DAPT falló (esperable si falta memoria/gpu en entorno limitado): {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### B. Extracción de Embeddings (Multi-Keyword)\n",
                "Usamos el script `extract.py` para procesar nuestro CSV de muestra y buscar \"Yape\", \"Yapear\", etc."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "OUTPUT_PARQUET = os.path.join(DATA_DEMO, \"embeddings_output.parquet\")\n",
                "KEYWORDS = [\"Yape\", \"Yapear\", \"Plin\"]\n",
                "\n",
                "print(f\"Extrayendo embeddings para {KEYWORDS}...\")\n",
                "\n",
                "try:\n",
                "    extract_embeddings(DATA_DEMO, OUTPUT_PARQUET, keywords=KEYWORDS, model_name=MODEL_NAME)\n",
                "    \n",
                "    if os.path.exists(OUTPUT_PARQUET):\n",
                "        df_results = pd.read_parquet(OUTPUT_PARQUET)\n",
                "        print(f\"\\n✅ Extracción completada. Vectores obtenidos: {len(df_results)}\")\n",
                "        print(df_results.head(3))\n",
                "    else:\n",
                "        print(\"⚠️ No se encontraron las palabras clave en la muestra de datos.\")\n",
                "        \n",
                "except Exception as e:\n",
                "    print(f\"Error en extracción masiva: {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Limpieza (Opcional)\n",
                "Podemos eliminar los datos temporales."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# shutil.rmtree(SANDBOX_DIR)\n",
                "print(\"Demo finalizada. Los archivos temporales están en 'demo_sandbox'.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}