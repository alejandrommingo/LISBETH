{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a07be77a",
   "metadata": {},
   "source": [
    "# Fase 3: Análisis de Subespacios Semánticos\n",
    "## \"De vectores a geometrías evolutivas\"\n",
    "\n",
    "**Objetivo Educativo**: Este notebook explica cómo transformamos listas de vectores (embeddings) en estructuras geométricas estables que nos permitan medir la evolución del significado de \"Yape\" en el tiempo.\n",
    "\n",
    "### Pasos de la Fase 3:\n",
    "1.  **Segmentación Temporal** (Fase 3.1 - *Implementado*): Dividir el tiempo en \"ventanas deslizantes\" para suavizar el ruido.\n",
    "2.  **Análisis de Dimensionalidad** (Fase 3.2 - *Pendiente*): Determinar cuántas dimensiones ($k$) son reales.\n",
    "3.  **Construcción de Subespacios** (Fase 3.3 - *Pendiente*): Aplicar SVD y alinamiento.\n",
    "4.  **Métricas Sociológicas** (Fase 3.4 - *Pendiente*): Calcular deriva y proyecciones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6e463a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Asegurar que podemos importar los módulos del proyecto\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "from src.analysis.temporal import TemporalSegmenter\n",
    "\n",
    "# Configuración de gráficos\n",
    "plt.style.use('seaborn-v0_8')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca718462",
   "metadata": {},
   "source": [
    "## Paso 1: Carga de Datos Reales\n",
    "En lugar de simular, cargaremos los embeddings reales extraídos en la Fase 2.\n",
    "Archivo: `data/embeddings_test_cli.parquet`\n",
    "Dimensiones esperadas:\n",
    "*   **Filas (N)**: Ocurrencias del término (Noticias).\n",
    "*   **Columnas (D)**: Dimensiones del vector (3072 para Concatenación de últimas 4 capas de BERT Base).\n",
    "\n",
    "**Importante**: Para el SVD, la matriz debe tener la forma $(N, D)$. Reduciremos la dimensionalidad $D$ basándonos en la varianza de $N$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f515d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar datos reales (Todos los disponibles)\n",
    "import glob\n",
    "\n",
    "# Buscar todos los parquets de embeddings\n",
    "search_patterns = [\n",
    "    'data/embeddings_*.parquet',\n",
    "    '../data/embeddings_*.parquet',\n",
    "    os.path.join(os.getcwd(), 'data', 'embeddings_*.parquet')\n",
    "]\n",
    "\n",
    "found_files = set()\n",
    "for pattern in search_patterns:\n",
    "    found_files.update(glob.glob(pattern))\n",
    "    \n",
    "# Filtrar 'anchors' que es otro tipo de archivo\n",
    "parquet_files = [f for f in found_files if 'anchors' not in f]\n",
    "parquet_files = sorted(list(parquet_files))\n",
    "\n",
    "if parquet_files:\n",
    "    print(f\"Archivos encontrados: {parquet_files}\")\n",
    "    dfs = []\n",
    "    for p in parquet_files:\n",
    "        try:\n",
    "            d = pd.read_parquet(p)\n",
    "            dfs.append(d)\n",
    "        except Exception as e:\n",
    "            print(f\"Error leyendo {p}: {e}\")\n",
    "            \n",
    "    df_real = pd.concat(dfs, ignore_index=True)\n",
    "    print(f\"Total registros cargados (Crudo): {len(df_real)}\")\n",
    "    \n",
    "    # --- AUDIT FIX: REMOVE DUPLICATES ---\n",
    "    # Embeddings arrays cannot be used for drop_duplicates directy.\n",
    "    # We create a hash for deduplication.\n",
    "    df_real['hash'] = df_real['embedding'].apply(lambda x: hash(x.tobytes()))\n",
    "    df_real = df_real.drop_duplicates(subset=['hash'])\n",
    "    print(f\"Total registros tras borrar duplicados: {len(df_real)}\")\n",
    "    \n",
    "    # Validar dimensiones\n",
    "    sample_emb = df_real.iloc[0]['embedding']\n",
    "    print(f\"Dimensión del embedding: {len(sample_emb)}\")\n",
    "    \n",
    "    matrix_stack = np.vstack(df_real['embedding'].values)\n",
    "    print(f\"Matriz de Embeddings: {matrix_stack.shape}\")\n",
    "    \n",
    "    df_demo = df_real.copy()\n",
    "    df_demo['date'] = pd.to_datetime(df_demo['date'])\n",
    "    \n",
    "    df_demo['date'].groupby(df_demo.date.dt.to_period('M')).count().plot(kind='bar', figsize=(12, 4), title=\"Distribución Total de Noticias\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"ERROR FATAL: No se encontraron archivos de embeddings.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852a32e4",
   "metadata": {},
   "source": [
    "## Paso 2: Segmentación Temporal (Rolling Windows)\n",
    "**Fase 3.1**\n",
    "\n",
    "En sociología, el significado no cambia de un día para otro. Si analizamos día a día, el ruido nos ciega. Si analizamos año a año, perdemos los detalles.\n",
    "\n",
    "**Solución**: Ventanas Deslizantes (*Rolling Windows*).\n",
    "*   **Tamaño (Window Size)**: 3 meses. (Para tener densidad suficiente).\n",
    "*   **Paso (Step)**: 1 mes. (Para ver la evolución suave).\n",
    "\n",
    "*Ejemplo*: Ene-Mar -> Feb-Abr -> Mar-May...\n",
    "\n",
    "Usaremos la clase `TemporalSegmenter` que hemos implementado en `src/analysis/temporal.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23535ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar el Segmentador\n",
    "segmenter = TemporalSegmenter(df_demo, date_column='date')\n",
    "\n",
    "# Generar Ventanas (3 meses, paso 1 mes, mínimo 50 datos)\n",
    "windows = list(segmenter.generate_windows(window_months=3, step_months=1, min_count=50))\n",
    "\n",
    "print(f\"Se han generado {len(windows)} ventanas temporales válidas.\")\n",
    "\n",
    "# Visualizar la cobertura de las ventanas\n",
    "window_labels = [w['label'] for w in windows]\n",
    "counts = [w['count'] for w in windows]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.barh(window_labels, counts, color='teal')\n",
    "plt.xlabel(\"Densidad (Número de Embeddings)\")\n",
    "plt.ylabel(\"Ventana Temporal\")\n",
    "plt.title(\"Densidad de Vectores por Ventana Deslizante (3m)\")\n",
    "plt.gca().invert_yaxis() # Invertir para ver cronológico de arriba a abajo\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73486506",
   "metadata": {},
   "source": [
    "### ¿Por qué es importante esto?\n",
    "Observa en el gráfico cómo cada ventana garantiza un mínimo de muestras. Esto es crucial para el siguiente paso: **SVD (Singular Value Decomposition)**. \n",
    "Para calcular las dimensiones latentes de forma fiable, necesitamos que la matriz $X_t$ (filas=noticias, col=dimensiones) sea suficientemente \"alta\" (muchas filas) respecto a su rango.\n",
    "\n",
    "Si alguna ventana tuviera muy pocos datos (ej. < 50), el algoritmo la descarta automáticamente para proteger la integridad científica del estudio.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a13920",
   "metadata": {},
   "source": [
    "## Paso 3: Análisis de Dimensionalidad (Horn's Parallel Analysis)\n",
    "**Fase 3.2**\n",
    "\n",
    "¿Cuántas \"dimensiones\" tiene el significado de Yape? ¿2? ¿5? ¿10? \n",
    "En lugar de decidir arbitrariamente, usamos el **Análisis Paralelo de Horn**.\n",
    "\n",
    "**Lógica**:\n",
    "1. Calculamos los \"autovalores\" (fuerza de cada dimensión) de nuestros datos reales.\n",
    "2. Generamos 20 matrices de **ruido aleatorio** puro con la misma forma.\n",
    "3. Solo aceptamos una dimensión si su fuerza es **significativamente mayor** (percentil 95) que la que produciría el azar.\n",
    "\n",
    "Usaremos `src.analysis.dimensionality.SubspaceAnalyzer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2faaa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.analysis.dimensionality import SubspaceAnalyzer\n",
    "\n",
    "# Ejecutar Horn's PA en la primera ventana para tener una referencia visual detallada\n",
    "if len(windows) > 0:\n",
    "    window_0 = windows[0]\n",
    "    data_matrix = np.vstack(window_0['data']['embedding'].values) \n",
    "    print(f\"Analizando ventana inicial: {window_0['label']} (Shape: {data_matrix.shape})\")\n",
    "\n",
    "    analyzer = SubspaceAnalyzer(random_state=42)\n",
    "    k_opt, real_sv, thresholds = analyzer.horns_parallel_analysis(data_matrix, num_simulations=20)\n",
    "    \n",
    "    # Calcular Varianza Explicada Normalizada (%)\n",
    "    real_eigenvals = real_sv ** 2\n",
    "    total_variance = np.sum(real_eigenvals)\n",
    "    explained_variance_ratio = real_eigenvals / total_variance * 100 \n",
    "    \n",
    "    print(f\"\\n>> Número Óptimo de Dimensiones (k) Ventana Inicial: {k_opt}\")\n",
    "    print(f\"Varianza explicada por los primeros {k_opt} componentes: {np.sum(explained_variance_ratio[:k_opt]):.2f}%\")\n",
    "\n",
    "    # Visualización (Scree Plot - VARIANZA EXPLICADA %)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    components = np.arange(1, len(real_sv) + 1)\n",
    "    limit = 20\n",
    "    \n",
    "    # IMPORTANTE: Plotting Ratio (%)\n",
    "    plt.plot(components[:limit], explained_variance_ratio[:limit], 'o-', label='Varianza Explicada (%)', color='blue')\n",
    "    \n",
    "    # Thresholds también se deben convertir a ratio aprox para visualización comparable (aunque Horn es sobre SV raw)\n",
    "    # Como fallback visual simple, escalamos los thresholds al mismo ratio\n",
    "    thresholds_ratio = (thresholds[:limit]**2 / total_variance) * 100\n",
    "    plt.plot(components[:limit], thresholds_ratio, 'o--', label='Ruido Aleatorio (95% Benchmark)', color='red', alpha=0.5)\n",
    "\n",
    "    plt.axvline(x=k_opt, color='green', linestyle=':', label=f'Corte Óptimo (k={k_opt})')\n",
    "    plt.title(f\"Scree Plot: Varianza Explicada por Componente (k={k_opt})\")\n",
    "    plt.xlabel(\"Componente Principal\")\n",
    "    plt.ylabel(\"Varianza Explicada (%)\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Varianza acumulada k={k_opt}: {np.sum(explained_variance_ratio[:k_opt]):.2f}%\")\n",
    "\n",
    "else:\n",
    "    print(\"NO SE GENERARON VENTANAS SUFICIENTES PARA EL ANÁLISIS.\")\n",
    "\n",
    "# --- NUEVO: Evolución de Dimensionalidad (K) en el Tiempo ---\n",
    "print(\"\\nCalculando Evolución de Dimensionalidad (K) por Ventana...\")\n",
    "k_evolution = []\n",
    "dates_k = []\n",
    "\n",
    "for w in windows:\n",
    "    mat = np.vstack(w['data']['embedding'].values)\n",
    "    # Solo ejecutar Horn si hay suficientes muestras para que sea estable\n",
    "    if mat.shape[0] > 30: \n",
    "        # OPTIMIZACIÓN: Solo 5 simulaciones para el loop temporal (veloz)\n",
    "        k_this, _, _ = analyzer.horns_parallel_analysis(mat, num_simulations=5) \n",
    "        k_evolution.append(k_this)\n",
    "        dates_k.append(w['label'])\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(dates_k, k_evolution, marker='o', linestyle='-', color='purple')\n",
    "plt.title(\"Dinámica de la Complejidad: Número de Dimensiones Latentes (k) en el Tiempo\")\n",
    "plt.ylabel(\"Dimensiones (k)\")\n",
    "plt.yticks(range(0, max(k_evolution)+2))\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b095035e",
   "metadata": {},
   "source": [
    "## Paso 4: Construcción y Alineamiento de Subespacios\n",
    "**Fase 3.3**\n",
    "\n",
    "Una vez que sabemos cuánto \"miden\" los datos (k) y en qué ventanas dividirlos, construimos los objetos geométricos finales.\n",
    "*   **SVD**: Extraemos los ejes principales.\n",
    "*   **Alineamiento de Procrustes**: Rotamos el eje de Febrero para que encaje lo mejor posible con Enero.\n",
    "    *   *Sin esto*: La \"Dimensión 1\" podría significar \"Utilidad\" en Enero y \"Riesgo\" en Febrero solo porque el algoritmo giró los vectores.\n",
    "    *   *Con esto*: Mantenemos la coherencia semántica.\n",
    "\n",
    "Usaremos `src.analysis.subspaces.SubspaceConstructor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdf1788",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.analysis.subspaces import SubspaceConstructor\n",
    "\n",
    "constructor = SubspaceConstructor()\n",
    "# Construimos subespacios (usando k=Horn's PA automático o fijando uno para visualización)\n",
    "# Para este demo, fijaremos k=3 para facilitar la inspección\n",
    "subspaces = constructor.build_subspaces(windows, fixed_k=3, align=True)\n",
    "\n",
    "print(f\"Se han construido {len(subspaces)} subespacios alineados temporalmente.\")\n",
    "\n",
    "if len(subspaces) > 1:\n",
    "    # Inspeccionar Evolución\n",
    "    dates = [s.label for s in subspaces]\n",
    "    eigenvals_evolution = np.array([s.eigenvalues for s in subspaces])\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    for i in range(3):\n",
    "        plt.plot(dates, eigenvals_evolution[:, i], marker='o', label=f'Dimensión {i+1}')\n",
    "\n",
    "    plt.title(\"Evolución de la Importancia (Eigenvalues) de las Dimensiones Latentes\")\n",
    "    plt.xlabel(\"Ventana Temporal\")\n",
    "    plt.ylabel(\"Varianza Explicada\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Verificación de Alineamiento (Similitud entre ventanas adyacentes)\n",
    "    similarities = []\n",
    "    for i in range(1, len(subspaces)):\n",
    "        # Similitud simple: traza del producto de bases alineadas\n",
    "        s_prev = subspaces[i-1]\n",
    "        s_curr = subspaces[i]\n",
    "        # Rango 0-1 (aproximado)\n",
    "        sim = np.trace(np.abs(np.dot(s_prev.basis, s_curr.basis.T))) / s_prev.k\n",
    "        similarities.append(sim)\n",
    "\n",
    "    plt.figure(figsize=(12, 3))\n",
    "    plt.plot(dates[1:], similarities, color='purple', marker='x')\n",
    "    plt.title(\"Estabilidad Semántica (Similitud Cosine entre Ventanas Adyacentes)\")\n",
    "    plt.ylim(0, 1.1)\n",
    "    plt.show()\n",
    "\n",
    "    # --- NUEVO: Deriva del Centroide (Desplazamiento del tema central) ---\n",
    "    print(\"\\nCalculando Deriva del Centroide (Euclidean Distance)...\")\n",
    "    centroid_drift = []\n",
    "    \n",
    "    for i in range(1, len(subspaces)):\n",
    "        c_prev = subspaces[i-1].centroid\n",
    "        c_curr = subspaces[i].centroid\n",
    "        # Distancia euclídea entre medias\n",
    "        dist = np.linalg.norm(c_curr - c_prev)\n",
    "        centroid_drift.append(dist)\n",
    "        \n",
    "    plt.figure(figsize=(12, 3))\n",
    "    plt.plot(dates[1:], centroid_drift, color='brown', marker='d', linestyle='-.')\n",
    "    plt.title(\"Desplazamiento del Centroide: ¿Cuánto se mueve el 'Tema Central' en el espacio?\")\n",
    "    plt.ylabel(\"Distancia Euclídea\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffbfa19",
   "metadata": {},
   "source": [
    "## Paso 5: Métricas Sociológicas\n",
    "**Fase 3.4**\n",
    "\n",
    "Finalmente, traducimos la geometría a respuestas.\n",
    "\n",
    "1.  **Deriva Semántica**: ¿Cuánto ha cambiado Yape? (Gráfico de línea descendente si hay cambios).\n",
    "2.  **Complejidad (Entropía)**: ¿Se ha vuelto más complejo? (Barras de entropía).\n",
    "3.  **Proyecciones Teóricas (HÍBRIDO)**: \n",
    "    *   **Contextual**: Significado en uso. \n",
    "    *   **Estático (Baseline)**: Significado de diccionario (técnico).\n",
    "    *   *Si Contextual > Estático*: Yape se usa como se diseñó. *Si Contextual < Estático*: Yape ha mutado.\n",
    "\n",
    "Usaremos `src.analysis.metrics.SociologicalMetrics`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901f668f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.analysis.metrics import SociologicalMetrics\n",
    "\n",
    "# Cargar Anclas para proyección\n",
    "anchors_path = 'data/anchors_embeddings.parquet'\n",
    "# Búsqueda robusta de anclas\n",
    "possible_anchors_paths = [\n",
    "    anchors_path, \n",
    "    '../' + anchors_path,\n",
    "    os.path.join(os.getcwd(), 'data', 'anchors_embeddings.parquet') \n",
    "]\n",
    "final_anchors_path = None\n",
    "for p in possible_anchors_paths:\n",
    "    if os.path.exists(p):\n",
    "        final_anchors_path = p\n",
    "        break\n",
    "\n",
    "if final_anchors_path:\n",
    "    anchors_df = pd.read_parquet(final_anchors_path)\n",
    "    print(f\"Anclas cargados: {len(anchors_df)}\")\n",
    "    \n",
    "    metrics_calc = SociologicalMetrics()\n",
    "\n",
    "    # 1. Deriva\n",
    "    drift_df = metrics_calc.calculate_drift(subspaces)\n",
    "    print(\"\\n--- Deriva Semántica (Primeras filas) ---\")\n",
    "    print(drift_df.head())\n",
    "\n",
    "    # 2. Entropía\n",
    "    entropy_df = metrics_calc.calculate_entropy(subspaces)\n",
    "\n",
    "    # 3. Proyecciones\n",
    "    proj_df = metrics_calc.calculate_projections(subspaces, anchors_df)\n",
    "    \n",
    "    # --- VISUALIZACIÓN FINAL ---\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(14, 18), sharex=True)\n",
    "\n",
    "    # A. DRIFT\n",
    "    axes[0].plot(drift_df['date'], drift_df['similarity'], color='crimson', marker='o')\n",
    "    axes[0].set_title(\"Estabilidad Semántica (Similitud 1.0 = Sin Cambios)\")\n",
    "    axes[0].set_ylabel(\"Similitud Cosine\")\n",
    "    axes[0].grid(True)\n",
    "\n",
    "    # B. ENTROPY\n",
    "    axes[1].bar(entropy_df['date'], entropy_df['entropy'], color='teal', alpha=0.6)\n",
    "    axes[1].set_title(\"Complejidad Semántica (Entropía de Eigenvalues)\")\n",
    "    axes[1].set_ylabel(\"Entropía (Shannon)\")\n",
    "    axes[1].grid(True)\n",
    "\n",
    "    # C. PROJECTIONS (CONTRAST)\n",
    "    dimensions = set([c.split('_')[1] for c in proj_df.columns if c.startswith('score_')])\n",
    "    \n",
    "    colors = {'contextual': 'blue', 'static': 'grey'}\n",
    "    styles = {'contextual': '-', 'static': '--'}\n",
    "    \n",
    "    for dim in ['funcional', 'social', 'afectiva']:\n",
    "        # Plot Contextual\n",
    "        col_ctx = f'score_{dim}_contextual'\n",
    "        if col_ctx in proj_df.columns:\n",
    "            axes[2].plot(proj_df['date'], proj_df[col_ctx], \n",
    "                        color=colors['contextual'], linestyle=styles['contextual'], marker='o', \n",
    "                        label=f'{dim.capitalize()} (Contextual)')\n",
    "                        \n",
    "        # Plot Static\n",
    "        col_sta = f'score_{dim}_static'\n",
    "        if col_sta in proj_df.columns:\n",
    "            axes[2].plot(proj_df['date'], proj_df[col_sta], \n",
    "                        color=colors['static'], linestyle=styles['static'], marker='x', alpha=0.5,\n",
    "                        label=f'{dim.capitalize()} (Baseline)')\n",
    "\n",
    "    axes[2].set_title(\"Proyección Teórica: Contextual (Uso) vs Estático (Baseline)\")\n",
    "    axes[2].set_ylabel(\"Alineamiento (Cosine)\")\n",
    "    axes[2].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    axes[2].grid(True)\n",
    "\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"ADVERTENCIA: No se encontraron anclas. No se pueden calcular proyecciones.\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
