# LISBETH: De la Billetera M√≥vil al Actor Social
## An√°lisis Computacional de la Representaci√≥n Medi√°tica de Yape en el Per√∫

**TFM - M√°ster en Big Data y Data Science | UNED**
* **Investigador**: Alejandro Mingo
* **Proyecto**: `LISBETH` (Legitimacy & Identity Semantic BERT Embedding Time-series Harvester)

---

## üìñ Descripci√≥n del Proyecto

**Lisbeth** es un sistema de investigaci√≥n computacional ("Laboratorio") dise√±ado para analizar la evoluci√≥n sem√°ntica de la aplicaci√≥n "Yape" en la prensa peruana (2016-2023). El sistema combina t√©cnicas avanzadas de **NLP (Modelos Transformadores Adaptados al Dominio)** con **Sociolog√≠a Digital** para cuantificar c√≥mo la marca ha transitado de ser una herramienta financiera a un "Actor Social" leg√≠timo.

El n√∫cleo metodol√≥gico reside en la correcci√≥n de la **Anisotrop√≠a** del espacio vectorial y el an√°lisis de **Subespacios Sem√°nticos** din√°micos, permitiendo medir matem√°ticamente conceptos abstractos como la "Deriva Sem√°ntica" y la "Proyecci√≥n Sociol√≥gica".

---

## üèóÔ∏è Arquitectura y Fases del Proyecto

El sistema se orquesta mediante una CLI maestra: `pipeline_manager.py`.

### ‚úÖ Fase 1: Data Harvesting (Recolector Granular)
*Infraestructura de recolecci√≥n de noticias resiliente.*

*   **Estrategia "Day x Media"**: A diferencia de scrapers tradicionales que hacen consultas masivas, Lisbeth itera **d√≠a por d√≠a** y **medio por medio** (ej. "Solo El Comercio el 12/03/2020"). Esto bypass-ea las limitaciones de retorno de GDELT (max 250 registros) y asegura una completitud hist√≥rica cercana al 100%.
*   **Fuentes H√≠bridas**: GDELT (primaria), Google News (backup), RSS (tiempo real).
*   **Resiliencia**:
    *   Manejo de "Soft 404s" y contenido renderizado por JS (Client-Side) mediante selectores CSS espec√≠ficos por dominio (`src/news_harvester/domains.py`).
    *   Fallback autom√°tico a la librer√≠a `trafilatura` para extracci√≥n de texto limpio.

### ‚úÖ Fase 2: Infraestructura NLP (La "F√°brica de Embeddings")
*Transformaci√≥n de texto en tensores matem√°ticos ajustados.*

#### 2.1 Model Management
El sistema soporta cualquier modelo de Hugging Face, pero est√° optimizado para modelos monoling√ºes en espa√±ol:
*   **`PlanTL-GOB-ES/roberta-large-bne`**: SOTA (State of the Art) entrenado por la Biblioteca Nacional de Espa√±a.
*   **`dccuchile/bert-base-spanish-wwm-uncased`** (BETO): Alternativa robusta y ligera.

#### 2.2 DAPT (Domain-Adaptive Pretraining)
Antes de extraer embeddings, el modelo base se somete a un "re-entrenamiento" ligero (**DAPT**) utilizando el corpus recolectado en Fase 1.
*   **Por qu√©**: Un modelo gen√©rico no entiende que "Yapear" es un verbo o que "Plin" es un competidor, no un sonido.
*   **Par√°metros**:
    *   MLM (Masked Language Modeling): Se ocultan aleatoriamente palabras del corpus peruano y el modelo aprende a predecirlas.
    *   Epochs: Configurable (default 3).

#### 2.3 Extracci√≥n de Embeddings Contextuales
Para cada menci√≥n de la palabra clave (ej. "Yape"):
1.  **Tokenizaci√≥n**: Se localiza la palabra en la oraci√≥n. Si se fragmenta en sub-tokens (`['Yap', '##ear']`), se aplica **Mean Pooling** para obtener un √∫nico vector.
2.  **Layer Strategy**: Se extraen las activaciones ocultas.
    *   **`penultimate`**: La capa anterior a la √∫ltima (mejor para representaciones geom√©tricas generales).
    *   **`last4_concat`**: Concatenaci√≥n de las √∫ltimas 4 capas (4096 dims para RoBERTa-large), capturando matices sint√°cticos y sem√°nticos profundos.

### ‚úÖ Fase 3: An√°lisis de Subespacios (El "Laboratorio Matem√°tico")
*Donde ocurre la magia sociol√≥gica.*

#### 3.1 Dual Anisotropy Correction
Los modelos de lenguaje sufren de "Anisotrop√≠a": todos los vectores tienden a ocupar un cono estrecho en el espacio, distorsionando las distancias (coseno).
Lisbeth implementa un protocolo estricto de comparaci√≥n:
1.  **RAW (Crudo)**: Embeddings tal cual salen del modelo.
2.  **CORRECTED (Corregido)**: Se calcula el **Vector Medio Global** ($\mu_{global}$) de todo el corpus y se resta de cada embedding ($v' = v - \mu_{global}$). Esto "centra" la nube de puntos y revela la verdadera estructura sem√°ntica interna.

#### 3.2 Subespacios Din√°micos
Se agrupan los embeddings en **Ventanas Deslizantes** (ej. Trimestrales) y se aplica **SVD (Singular Value Decomposition)** para hallar los ejes principales de significado en ese periodo.

#### 3.3 M√©tricas
*   **Semantic Drift**: Distancia Grassmanniana entre el subespacio del tiempo $t$ y el tiempo $t+1$. Mide cu√°nto ha cambiado el significado.
*   **Entrop√≠a**: Dispersi√≥n de los valores singulares. Alta entrop√≠a = Significado difuso/polis√©mico.
*   **Proyecci√≥n de Anclas**: Se definen vectores te√≥ricos (ej. "Seguridad", "Comunidad") y se mide matem√°ticamente cu√°nto se acerca el concepto "Yape" a ellos.

### ‚úÖ Fase 4: Reportes Autom√°ticos
Generaci√≥n de Notebooks y Gr√°ficos (Heatmaps, Series Temporales) que comparan visualmente las condiciones RAW vs CORRECTED para validar los hallazgos.

---

## üöÄ Gu√≠a Exhaustiva de Par√°metros y Ejecuci√≥n

El script `pipeline_manager.py` es el punto de entrada √∫nico.

### 0. Configuraci√≥n Inicial
```bash
# Definir lista de medios (disponible en repo)
cat data/media_list.csv
# name,domain,type
# elcomercio,elcomercio.pe,national
# ...
```

### 1. Descarga de Modelos
Pre-descarga los modelos para evitar latencia o errores de red durante el proceso.
```bash
python pipeline_manager.py phase2 download-models \
    --models "dccuchile/bert-base-spanish-wwm-uncased" "PlanTL-GOB-ES/roberta-large-bne"
```

### 2. Fase 1: Recolecci√≥n (Harvesting)
**Par√°metros Clave**:
*   `--pipeline granular`: (Impl√≠cito en l√≥gica interna) Activa el loop "Day x Media".
*   `--media-list`: Ruta al CSV de medios. Si se omite, busca en todo GDELT (menos exhaustivo).
*   `--keyword`: Palabras a rastrear.

```bash
python pipeline_manager.py phase1 \
    --keyword "Yape" "Yapear" \
    --from 2020-01-01 --to 2021-01-01 \
    --media-list data/media_list.csv \
    --output data/raw_news_2020.csv
```

### 3. Fase 2: Procesamiento NLP

#### Paso 3.1: DAPT (Opcional pero Recomendado)
Entrena el modelo base sobre tu data.
*   `--model`: Modelo base de HuggingFace.
*   `--epochs`: 3 suele ser suficiente para adaptaci√≥n ligera.

```bash
python pipeline_manager.py phase2 dapt \
    --data data/raw_news_2020.csv \
    --output models/lisbeth-adapted-2020 \
    --model "dccuchile/bert-base-spanish-wwm-uncased" \
    --epochs 3
```

#### Paso 3.2: Extracci√≥n
Genera el dataset vectorial.
*   `--dapt_model`: Ruta al modelo entrenado en 3.1.
*   `--model`: Modelo base (se usa para generar la l√≠nea base comparativa).

```bash
python pipeline_manager.py phase2 extract \
    --data_dir data/raw_news_dir_2020 \
    --output data/embeddings_2020.csv \
    --model "dccuchile/bert-base-spanish-wwm-uncased" \
    --dapt_model models/lisbeth-adapted-2020
```

### 4. Fase 3: An√°lisis de Subespacios
Ejecuta el c√°lculo masivo de m√©tricas. No requiere par√°metros complejos, ya que la configuraci√≥n cient√≠fica (ventanas, anclas, estrategias) se define en `src/phase3/schemas.py` o se infiere.
*   **Output**: Genera una estructura de carpetas `artifacts/` con subespacios `.npz` y un CSV resumen `phase3_results.csv`.

```bash
python pipeline_manager.py phase3 \
    --input data/embeddings_2020.csv \
    --output-dir results/analysis_2020
```

### 5. Fase 4: Reporte
Genera el entregable final.
*   Crea un Notebook de Jupyter (`report.ipynb`) en la carpeta de destino con todas las gr√°ficas pre-cargadas.

```bash
python pipeline_manager.py phase4 \
    --input results/analysis_2020/phase3_results.csv \
    --output_dir results/final_report_2020
```

---

## üìÇ Estructura del Repositorio

```
LISBETH/
‚îú‚îÄ‚îÄ academic/               # Templates de reportes metodol√≥gicos
‚îú‚îÄ‚îÄ data/                   # Datos (Gitignored, salvo media_list.csv)
‚îÇ   ‚îî‚îÄ‚îÄ media_list.csv      # Cat√°logo de medios peruanos
‚îú‚îÄ‚îÄ execution_test/         # Artefactos de validaci√≥n (Run de prueba)
‚îú‚îÄ‚îÄ notebooks/              # Demos interactivos
‚îú‚îÄ‚îÄ models/                 # Modelos (Gitignored)
‚îú‚îÄ‚îÄ scripts/                # Utilidades (Generator de assets)
‚îú‚îÄ‚îÄ src/                    # C√≥digo Fuente
‚îÇ   ‚îú‚îÄ‚îÄ news_harvester/     # L√≥gica scraping (Domains, Selectors)
‚îÇ   ‚îú‚îÄ‚îÄ nlp/                # L√≥gica DAPT y tensores
‚îÇ   ‚îú‚îÄ‚îÄ phase3/             # Matem√°ticas (SVD, Grassman, Procrustes)
‚îÇ   ‚îî‚îÄ‚îÄ phase4/             # Reporting logic
‚îú‚îÄ‚îÄ pipeline_manager.py     # CLI Maestro
‚îî‚îÄ‚îÄ README.md               # Este archivo
```

---
**Lisbeth v2.0 - Enero 2026**
