# LISBETH: De la Billetera M√≥vil al Actor Social
## An√°lisis Computacional de la Representaci√≥n Medi√°tica de Yape en el Per√∫

**TFM - M√°ster en Big Data y Data Science | UNED**
* **Investigador**: Alejandro Mingo
* **Proyecto**: `LISBETH` (Legitimacy & Identity Semantic BERT Embedding Time-series Harvester)

---

## üìñ Descripci√≥n del Proyecto

**Lisbeth** es un sistema de investigaci√≥n computacional dise√±ado para analizar c√≥mo la aplicaci√≥n "Yape" ha trascendido su funci√≥n financiera para convertirse en un **Actor Social** en la cultura peruana. 

El proyecto combina **Sociolog√≠a Digital** y **Procesamiento de Lenguaje Natural (NLP)** para rastrear la evoluci√≥n sem√°ntica de la marca en la prensa nacional (2016-2023), identificando c√≥mo los medios construyen y transforman su legitimidad (de la "innovaci√≥n funcional" a la "solidaridad cotidiana").

---

## üèóÔ∏è Arquitectura y Fases del Proyecto

El desarrollo se estructura en fases secuenciales que transforman datos no estructurados en conocimiento sociol√≥gico.

### ‚úÖ Fase 1: Data Harvesting (Recolector de Noticias)
*Infraestructura de recolecci√≥n masiva y curaci√≥n de corpus.*

*   **Fuentes H√≠bridas**: Integraci√≥n de **GDELT** (hist√≥rico profundo), **Google News** y **RSS** directos.
*   **Cobertura**: +30 medios peruanos (El Comercio, La Rep√∫blica, Gesti√≥n, RPP, etc.).
*   **Capacidades T√©cnicas**:
    *   **Multi-Keyword Targeting**: Rastreo simult√°neo de variantes (`Yape`, `Yapear`, `Yapeo`, `Plin`).
    *   **Daily Chunking**: Algoritmo de segmentaci√≥n diaria para maximizar la recuperaci√≥n de datos hist√≥ricos (superando l√≠mites de API).
    *   **WAF Bypass**: Navegaci√≥n simulada para extraer contenido de sitios protegidos (Client-Side Rendering).
    *   **Relevance Scoring**: Clasificaci√≥n autom√°tica de art√≠culos seg√∫n la densidad terminol√≥gica.

### ‚úÖ Fase 2: Infraestructura NLP
*Adaptaci√≥n de modelos y vectorizaci√≥n sem√°ntica.*

*   **Core Model**: Modelos Transformadores del Estado del Arte (SOTA) en espa√±ol (`PlanTL-GOB-ES/roberta-large-bne` o `xlm-roberta`).
*   **DAPT (Domain-Adaptive Pretraining)**: Re-entrenamiento del modelo base con el corpus period√≠stico peruano recolectado para "ense√±arle" terminolog√≠a local y jerga financiera espec√≠fica.
*   **Subword Mean Pooling**: Estrategia matem√°tica para reconstruir vectores de palabras fragmentadas por el tokenizador (ej: `['Yap', '##ear']` $\rightarrow$ `Yapear`).
*   **Extracci√≥n de Embeddings Contextuales**: Generaci√≥n de representaciones vectoriales densas para cada ocurrencia de la marca, capturando el significado exacto seg√∫n su contexto de uso.

### üöß Fase 3: An√°lisis de Subespacios Sem√°nticos (En Progreso)
*Modelado matem√°tico de la evoluci√≥n.*
*   An√°lisis de Componentes Principales (PCA) y SVD sobre ventanas temporales.
*   Detecci√≥n de Deriva Sem√°ntica (*Semantic Drift*).
*   Proyecci√≥n de Marcos Te√≥ricos (Confianza, Inclusi√≥n, Riesgo).

---

## üöÄ Gu√≠a de Uso R√°pida

### 1. Instalaci√≥n
```bash
git clone https://github.com/alejandrommingo/LISBETH.git
cd LISBETH
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

### 2. Recolecci√≥n de Datos (Harvester)
Descargar noticias hist√≥ricas de medios peruanos:
```bash
# Ejemplo: Descargar noticias de 2020 a 2021 sobre Yape
PYTHONPATH=src python -m news_harvester prototype \
    --keyword "Yape" "Yapear" \
    --from 2020-01-01 --to 2021-01-01 \
    --media all \
    --output data/yape_2020.csv
```

### 3. Pipeline NLP
Ejecutar las herramientas de procesamiento de lenguaje:

**A. Adaptaci√≥n al Dominio (DAPT):**
Entrenar el modelo con el texto descargado para mejorar su comprensi√≥n:
```bash
python src/cli.py dapt --data data/corpus.txt --output models/lisbeth-roberta-adapted --epochs 3
```

**B. Extracci√≥n de Embeddings:**
Generar la base de datos vectorial para an√°lisis:
```bash
python src/cli.py extract \
    --data_dir data \
    --keywords Yape Yapear Plin \
    --output data/embeddings_final.parquet
```

### 4. Demo Educativa
Explora el funcionamiento interno paso a paso:
```bash
jupyter notebook notebooks/phase2_demo.ipynb
```

---

## üìÇ Estructura del Repositorio

```
LISBETH/
‚îú‚îÄ‚îÄ academic/           # Documentaci√≥n te√≥rica (TFM Intro, Metdolog√≠a)
‚îú‚îÄ‚îÄ data/               # Corpus crudo y Datasets (Ignorados por git)
‚îú‚îÄ‚îÄ models/             # Checkpoints de modelos NLP (Ignorados por git)
‚îú‚îÄ‚îÄ notebooks/          # Demos y experimentos (Jupyter)
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ data/           # L√≥gica de scraping y curaci√≥n
‚îÇ   ‚îú‚îÄ‚îÄ nlp/            # Modelos, DAPT y Extracci√≥n
‚îÇ   ‚îú‚îÄ‚îÄ utils/          # Herramientas auxiliares
‚îÇ   ‚îî‚îÄ‚îÄ cli.py          # Punto de entrada unificado
‚îú‚îÄ‚îÄ tests/              # Tests unitarios y de integraci√≥n
‚îî‚îÄ‚îÄ README.md           # Documentaci√≥n del proyecto
```

---

**Estado del Proyecto**: Fase 2 Completada (Diciembre 2025).
