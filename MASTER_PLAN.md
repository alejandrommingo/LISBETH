# Plan Maestro: An√°lisis de Yape como Actor Social

Este documento establece la hoja de ruta integral para la investigaci√≥n "De la billetera m√≥vil al actor social", detallando roles, fases y tareas espec√≠ficas para garantizar una ejecuci√≥n robusta y cient√≠ficamente v√°lida.

## 1. Definici√≥n de Roles y Responsabilidades

Para asegurar la calidad en cada etapa del pipeline, definimos los siguientes roles l√≥gicos (que pueden ser ejecutados por una o varias personas/agentes):

### üõ†Ô∏è Role: Data Engineer (Ingeniero de Datos)
**Responsabilidad**: Garantizar la disponibilidad, calidad y completitud del corpus de noticias.
*   Mantenimiento de `Lisbeth News Harvester`.
*   Implementaci√≥n de soporte para m√∫ltiples queries.
*   Limpieza y preprocesamiento de texto (normalizaci√≥n, eliminaci√≥n de ruido).
*   Gesti√≥n del almacenamiento de datos y embeddings.

### üß† Role: NLP Engineer (Ingeniero de PNL)
**Responsabilidad**: Transformar texto en representaciones matem√°ticas precisas.
*   Selecci√≥n y validaci√≥n de modelos de lenguaje (BERT/RoBERTa).
*   Implementaci√≥n del pipeline de extracci√≥n de embeddings contextuales (Token-level).
*   Fine-tuning de modelos (si fuera necesario por especificidad del dominio).

### üìê Role: Data Scientist (Cient√≠fico de Datos)
**Responsabilidad**: Modelado matem√°tico y c√°lculo de m√©tricas.
*   Implementaci√≥n de algoritmos de reducci√≥n de dimensionalidad (PCA/SVD).
*   Ejecuci√≥n de An√°lisis Paralelo de Horn y Bootstrapping para estabilidad.
*   C√°lculo de m√©tricas complejas (Deriva Sem√°ntica, Entrop√≠a, Proyecci√≥n).

### üîé Role: Researcher (Investigador Principal)
**Responsabilidad**: Definici√≥n te√≥rica e interpretaci√≥n de resultados.
*   Definici√≥n de "Vectores Ancla" (listas de palabras semilla para cada frame).
*   Validaci√≥n sem√°ntica de los subespacios hallados.
*   Redacci√≥n de hallazgos y vinculaci√≥n con la teor√≠a.

---

## 2. Plan de Ejecuci√≥n Detallado

### Fase 1: Expansi√≥n y Refinamiento de Datos (Data Engineer)
**Objetivo**: Capturar todas las variaciones de la marca para no perder data relevante.

*   [x] **Implementar Multi-Query Support**: Modificar el harvester para aceptar listas de keywords (`["Yape", "Yapear", "Yapeo", "Yapame"]`).
*   [x] **Implementar Daily Chunking**: Modificar el harvester para que **siempre** divida las consultas en intervalos diarios (d√≠a a d√≠a) para asegurar la m√°xima exhaustividad, independientemente del rango total.
*   [x] **Soluci√≥n Ligera para Renderizado JS**: Implementar un mecanismo (ej. Playwright o similar optimizado) para extraer texto de medios con Client-Side Rendering (La Rep√∫blica, etc.) que actualmente devuelven "200 OK" pero sin contenido. **Cr√≠tico antes de avanzar**.
*   [x] **Recolecci√≥n Hist√≥rica Completa (Re-run)**: Ejecutar barrido **1 Enero 2020 - 1 Enero 2021** con Daily Chunking y soporte JS.

### Fase 2: Infraestructura NLP (NLP Engineer)
**Objetivo**: Convertir el corpus en una base de datos de vectores contextuales robustos.

*   [x] **Selecci√≥n de Modelo Principal**: Utilizar `PlanTL-GOB-ES/roberta-large-bne` (Encoder Bidireccional SOTA en espa√±ol).
    *   **Modelo de Contraste**: Usar `bertin-project/bertin-roberta-base-spanish` para validar robustez.
    *   **Nota**: Se descarta GPT (Decoder) por inadecuaci√≥n para representaci√≥n sem√°ntica bidireccional.
*   [x] **Adaptaci√≥n al Dominio (DAPT)**: Realizar *Continued Pretraining* del modelo sobre el corpus de prensa peruana para mejorar la representaci√≥n de terminolog√≠a local ("Yapear", "Plin").
*   [x] **Pipeline de Tokenizaci√≥n**: Implementar estrategia de **Subword Pooling** (promedio de sub-tokens) para manejar correctamente la fragmentaci√≥n (ej. `['Yape', '##ar']` -> `Yapear`).
*   [x] **Extracci√≥n de Embeddings**: Generar tensores para cada ocurrencia de la marca.
    *   **Estrategia**: Concatenar √∫ltimas 4 capas (o investigar pen√∫ltima) + Normalizaci√≥n (Whitening/Centering).
*   [x] **Almacenamiento**: Guardar los vectores con metadatos (fecha, medio, oraci√≥n original) en formato eficiente (ej. Parquet).

### Fase 3: An√°lisis de Subespacios Sem√°nticos (En Progreso)
**Objetivo**: Modelar la evoluci√≥n del significado de la marca a lo largo del tiempo mediante t√©cnicas algebraicas.

#### Sub-fase 3.1: Estrategia de Segmentaci√≥n Temporal (Data Scientist)
**Objetivo**: Preparar los datos para un an√°lisis evolutivo robusto, evitando el ruido de las fluctuaciones diarias.
*   [ ] **Implementar Rolling Windows**: Crear generador de ventanas deslizantes configurables (ej. Tama√±o: 3 meses, Paso: 1 mes) para suavizar tendencias.
*   [ ] **Filtrado Din√°mico de Vocabulario**: Asegurar que solo t√©rminos relevantes y persistentes en la ventana temporal sean considerados (min_frequency per window).
*   [ ] **Validaci√≥n de Densidad**: Verificar que cada ventana tenga suficiente densidad de "keywords" para un an√°lisis estad√≠sticamente significativo.

#### Sub-fase 3.2: An√°lisis de Estabilidad y Dimensionalidad (Data Scientist)
**Objetivo**: Determinar matem√°ticamente cu√°ntas dimensiones ($k$) son necesarias para representar la realidad latente sin sobreajuste.
*   [ ] **An√°lisis Paralelo de Horn**: Implementar test de permutaci√≥n para distinguir se√±al de ruido aleatorio.
*   [ ] **Bootstrapping de Estabilidad**: Evaluar la robustez de los autovalores mediante remuestreo con reemplazo.
*   [ ] **Selecci√≥n de $k$ √ìptimo**: Definir criterio de corte autom√°tico para cada ventana temporal.

## 3.3 Subspace Construction (Data Scientist)
- [ ] **Architecture Refactor**: Split into `scripts/run_phase3_pipeline.py` (CLI) and `notebooks/phase3_analysis.ipynb` (Viewer).
- [ ] Implement SVD decomposition on centered embeddings <!-- id: 45 -->
- [ ] Implement Orthogonal Procrustes for temporal alignment <!-- id: 46 -->
- [ ] Validate alignment stability with synthetic data <!-- id: 47 -->

## 3.4 Sociological Metrics (Researcher + Data Scientist)
- [ ] **Methodology Upgrade**: Implement **Gram-Schmidt Orthogonalization** for Anchors (`metrics.py`).
- [ ] Calculate Semantic Drift (Cosine Distance $t$ vs $t+1$) <!-- id: 47 -->
- [ ] Calculate Theoretical Projections (Heatmap of Basis vs Orthogonal Anchors) <!-- id: 48 -->
- [ ] Calculate Semantic Entropy (Volume of meaning) <!-- id: 49 -->
la "ambig√ºedad" o "riqueza" del significado.
*   [ ] **Proyecci√≥n de Marcos (Frame Projection)**: Proyectar los vectores de la marca sobre los ejes definidos por las Anclas Contextuales (Confianza, Inclusi√≥n, Riesgo) extra√≠das en Fase 2.

### Fase 4: Interpretaci√≥n y Visualizaci√≥n (Researcher + Data Scientist)
**Objetivo**: Traducir n√∫meros a narrativa sociol√≥gica.

*   [ ] **Definici√≥n de Anclas**: Implementar **Estrategia H√≠brida**:
    *   *Baseline*: Vectores est√°ticos (Capa 0) de las keywords.
    *   *Contextual*: Centroides de "Oraciones Protot√≠picas" (del archivo `data/dimensiones_ancla.json`) procesadas por el modelo (√∫ltimas 4 capas).
*   [ ] **Validaci√≥n ("Ground Truth")**: Etiquetado manual de un subconjunto de datos para validar alineamiento de frames y valencia.
*   [ ] **Visualizaci√≥n Evolutiva**: Graficar m√©tricas con bandas de error (IC).
*   [ ] **Redacci√≥n de Resultados**: Integrar hallazgos validando hip√≥tesis.

---

## 3. Pr√≥ximos Pasos Inmediatos (Sprint Actual)

1.  **Data Engineer**: Actualizar `Lisbeth` para soportar m√∫ltiples keywords (ej. "Yape", "Yapear").
2.  **Researcher**: Validar la lista de variantes de la marca a rastrear.
