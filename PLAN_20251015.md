# Lisbeth · Plan de trabajo · 15-10-2025

## Contexto y objetivo
- Analizar la evolución de la reputación de Yape (billetera digital del BCP) en la prensa peruana.
- Construir un prototipo que descargue noticias entre el 01-03-2020 y el 01-05-2020 con la palabra clave "Yape" para validar el pipeline.
- Generar una tabla estructurada con título, periódico, texto plano y fecha exacta de publicación.

## Entregables clave
- [ ] Script/CLI capaz de consultar GDELT, descargar HTML, extraer texto plano y volcarlo a una tabla (CSV/Parquet) con los campos requeridos.
- [ ] Dataset prototipo con el rango 2020-03-01 → 2020-05-01 almacenado en `data/`.
- [ ] Pruebas automatizadas (unitarias) que cubran al menos la extracción de texto y la transformación a registros estructurados.
- [ ] Documentación en `README.md` (o anexo) que detalle cómo ejecutar el prototipo y extensiones hacia 2019-2022.

## Hoja de ruta detallada
1. **Preparación del entorno**
   - [x] Añadir dependencias necesarias (pandas u otra librería tabular, quizá readability) y actualizar manifest.
   - [x] Ampliar configuración (`Settings`) para parámetros del prototipo (rango de fechas por defecto, ruta de salida).

2. **Procesamiento de HTML → texto**
   - [x] Implementar un módulo `processing.text` con utilidades para limpiar HTML (BeautifulSoup, normalización de espacios, eliminación de scripts/ads).
   - [x] Añadir tests unitarios con fixtures HTML representativos.

3. **Modelo de datos y almacenamiento**
   - [x] Definir un modelo (dataclass / pydantic) `NewsRecord` con campos `title`, `source`, `published_at`, `plain_text`, `url`.
   - [x] Implementar adaptadores para convertir `Article` (GDELT) + HTML en `NewsRecord`.
   - [x] Crear módulo de persistencia (por ejemplo `storage/table.py`) que escriba listas de `NewsRecord` a CSV/Parquet.

4. **Pipeline del prototipo**
   - [x] Construir función `run_prototype(keyword, start, end, ...)` que encadene GDELT → descarga HTML → limpieza → tabla.
   - [x] Integrar la función en el CLI con un subcomando específico (`prototype` o bandera en `fetch`).
   - [x] Depurar bloques de navegación y etiquetas promocionales del texto plano antes de persistir (evita ruido en CSV).
   - [ ] Registrar logs/resumen de ejecución (mejorar trazabilidad y métricas básicas).

5. **Validación y documentación**
   - [x] Ejecutar el pipeline para el rango objetivo y verificar tamaño/calidad (17 registros obtenidos en `data/yape_20200301_20200501.csv`).
   - [x] Añadir instrucciones de uso en el README y anotar limitaciones (ver sección de subcomando `prototype`).
   - [x] Identificar pasos siguientes para escalar a 2019-2022 (ver Backlog de riesgos e innovación).

## Backlog de riesgos e innovación
- [ ] Implementar detección de duplicados entre artículos (riesgo: noticias replicadas en varios medios o versiones).
- [ ] Añadir fallback a Wayback Machine cuando la descarga HTML falle (riesgo: enlaces rotos o paywalls).
- [ ] Registrar métricas de tasa de solicitudes y aplicar throttling adaptativo (riesgo: bloqueo por parte de los medios).
- [ ] Diseñar extractores específicos por dominio para mejorar la calidad del texto (innovación: mayor precisión en análisis semántico).
- [ ] Incorporar análisis de sentimiento o tópicos sobre el texto plano (innovación: medir reputación automáticamente).
- [ ] Automatizar la ejecución periódica y almacenamiento incremental (innovación/escala).
- [ ] Normalizar los nombres de los medios (dominio → nombre legible) para análisis reputacional.
- [ ] Persistir metadatos adicionales de GDELT (imágenes, url_mobile) para enriquecer visualizaciones.

## Riesgos y mitigaciones
- **Paywalls / contenido faltante** → aproximar con Wayback Machine, marcar registros incompletos.
- **Cambios en HTML entre medios** → diseñar extractores por dominio con fallback genérico.
- **Límites GDELT (250 registros)** → paginar por semanas o usar `offset`.
- **Calidad del texto** → limpiar caracteres especiales y normalizar UTF-8.

## Métricas de éxito
- Prototipo genera ≥80% de los artículos esperados en el rango sin errores críticos.
- Pruebas unitarias cubren casos normales y edge cases de extracción.
- El dataset resultante es reutilizable para análisis temporal (formato tabular estándar).
